CLI versions of major models (Gemini CLI, Claude Code, OpenCode, Codex) — use the same power as web apps but in your terminal.

File system access (read/write) — AI can read and update your project files, notes, Obsidian vault, etc., so no more copy/paste.

Persistent local context files (e.g., gemini.md, claude.md) — a single source-of-truth that loads every session so context isn’t scattered across browser chats.

Multi-AI, single-project workflow — run Gemini, Claude, Codex in the same directory and they all see the same project context.

Agents & sub-agents — spawn specialist agents to parallelize work (research, critique, drafting) while keeping the main conversation uncluttered.

Fresh context per agent — sub-agents get separate context windows (huge token buffers) to avoid bias and bloat.

Output styles / personas — create reusable system prompts (personas) to shift voice and behavior instantly.

Headless & automation-friendly — run AIs in headless mode, script prompts, and build automated workflows.

Local model support — run private/local LLMs (e.g., Llama) for privacy or offline use.

Tool & script integration — execute bash, Python, run scripts, or call external tools directly from the terminal.

Project session management — resume sessions, export/share conversation history, timeline/restore states.

Git integration & versioning — treat drafts/notes as code: commit, track history, and revert changes.

Critic agents & review loops — dedicated agents that roast/critique work to improve quality and prevent AI agreeableness.

Security controls & permission prompts — fine-grained permissions; but be mindful—AI can access your network/files (zero-trust recommended).

Concurrent multi-agent orchestration — run multiple agents simultaneously for faster, specialized parallel work.